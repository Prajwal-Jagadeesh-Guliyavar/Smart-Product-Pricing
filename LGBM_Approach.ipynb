{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/holy_SHIT/ML_AWS_CHALLENGE/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-12 23:09:52.950353: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n",
      "Loading and preprocessing data...\n",
      "Data preprocessing complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>price</th>\n",
       "      <th>log_price</th>\n",
       "      <th>ipq</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33127</td>\n",
       "      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n",
       "      <td>4.89</td>\n",
       "      <td>1.773256</td>\n",
       "      <td>6</td>\n",
       "      <td>la victoria green taco sauce mild 12 ounce pac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198967</td>\n",
       "      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n",
       "      <td>13.12</td>\n",
       "      <td>2.647592</td>\n",
       "      <td>4</td>\n",
       "      <td>salerno cookies the original butter cookies 8 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261251</td>\n",
       "      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n",
       "      <td>1.97</td>\n",
       "      <td>1.088562</td>\n",
       "      <td>6</td>\n",
       "      <td>bear creek hearty soup bowl creamy chicken wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55858</td>\n",
       "      <td>Item Name: Judee’s Blue Cheese Powder 11.25 oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n",
       "      <td>30.34</td>\n",
       "      <td>3.444895</td>\n",
       "      <td>1</td>\n",
       "      <td>judees blue cheese powder 1125 oz glutenfree a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>292686</td>\n",
       "      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n",
       "      <td>66.49</td>\n",
       "      <td>4.211979</td>\n",
       "      <td>12</td>\n",
       "      <td>kedem sherry cooking wine 127 ounce 12 per cas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                    catalog_content  \\\n",
       "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
       "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
       "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
       "3      55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n",
       "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
       "\n",
       "                                          image_link  price  log_price  ipq  \\\n",
       "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89   1.773256    6   \n",
       "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12   2.647592    4   \n",
       "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97   1.088562    6   \n",
       "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34   3.444895    1   \n",
       "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49   4.211979   12   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  la victoria green taco sauce mild 12 ounce pac...  \n",
       "1  salerno cookies the original butter cookies 8 ...  \n",
       "2  bear creek hearty soup bowl creamy chicken wit...  \n",
       "3  judees blue cheese powder 1125 oz glutenfree a...  \n",
       "4  kedem sherry cooking wine 127 ounce 12 per cas...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib # For saving the model\n",
    "\n",
    "print(\"Libraries imported.\")\n",
    "\n",
    "# --- 1. Data Loading and Preprocessing ---\n",
    "def load_and_preprocess_data(data_path):\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # --- Feature Engineering from previous notebook ---\n",
    "    df['log_price'] = np.log1p(df['price'])\n",
    "\n",
    "    def extract_ipq(text):\n",
    "        patterns = [r'pack of (\\d+)', r'(\\d+)\\s*per case', r'\\((\\d+)\\s*count\\)', r'pack\\s*(\\d+)', r'(\\d+)\\s*pack']\n",
    "        text_lower = text.lower()\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text_lower)\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "        return 1\n",
    "    df['ipq'] = df['catalog_content'].apply(extract_ipq)\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'item name:', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'bullet point \\d+:', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'value:', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'unit:', '', text, flags=re.IGNORECASE)\n",
    "        text = text.lower().replace('\\n', ' ')\n",
    "        text = re.sub(r'[^a-z0-9 ]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    df['cleaned_text'] = df['catalog_content'].apply(clean_text)\n",
    "\n",
    "    print(\"Data preprocessing complete.\")\n",
    "    return df\n",
    "\n",
    "df = load_and_preprocess_data('student_resource/dataset/train.csv')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed text embeddings from artifacts/text_embeddings.npy...\n",
      "Text embeddings loaded with shape: (75000, 384)\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Generate Text Embeddings ---\n",
    "\n",
    "# Define the model from sentence-transformers and the output file path\n",
    "TEXT_EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "TEXT_EMBEDDINGS_FILE = 'artifacts/text_embeddings.npy'\n",
    "\n",
    "# Create artifacts directory if it doesn't exist\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "\n",
    "if not os.path.exists(TEXT_EMBEDDINGS_FILE):\n",
    "    print(\"Text embeddings file not found. Generating new embeddings...\")\n",
    "    print(f\"Loading model: {TEXT_EMBEDDING_MODEL}\")\n",
    "    # Load the pre-trained model\n",
    "    model = SentenceTransformer(TEXT_EMBEDDING_MODEL)\n",
    "\n",
    "    # Generate embeddings. This may take a few minutes.\n",
    "    print(\"Generating text embeddings for all 75,000 items...\")\n",
    "    text_embeddings = model.encode(\n",
    "        df['cleaned_text'].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        batch_size=128 # Adjust batch size based on your RAM\n",
    "    )\n",
    "\n",
    "    # Save the embeddings to a file for future use\n",
    "    print(f\"Embeddings generated with shape: {text_embeddings.shape}\")\n",
    "    np.save(TEXT_EMBEDDINGS_FILE, text_embeddings)\n",
    "    print(f\"Text embeddings saved to {TEXT_EMBEDDINGS_FILE}\")\n",
    "else:\n",
    "    print(f\"Loading pre-computed text embeddings from {TEXT_EMBEDDINGS_FILE}...\")\n",
    "    text_embeddings = np.load(TEXT_EMBEDDINGS_FILE)\n",
    "    print(f\"Text embeddings loaded with shape: {text_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed image embeddings from artifacts/image_embeddings.npy...\n",
      "Image embeddings loaded with shape: (75000, 2048)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add src directory to path to import utils\n",
    "src_path = os.path.abspath('student_resource/src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "from challenge_utils import download_images\n",
    "\n",
    "IMAGE_EMBEDDINGS_FILE = 'artifacts/image_embeddings.npy'\n",
    "IMAGES_DIR = 'student_resource/images'\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "# Add image_path column to dataframe\n",
    "df['image_path'] = df['image_link'].apply(lambda url: os.path.join(IMAGES_DIR, Path(url).name))\n",
    "\n",
    "if not os.path.exists(IMAGE_EMBEDDINGS_FILE):\n",
    "    print(\"Image embeddings file not found. Generating new embeddings...\")\n",
    "\n",
    "    # 1. Download all images.\n",
    "    print(\"Verifying all images are downloaded...\")\n",
    "    os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "    image_urls = df['image_link'].tolist()\n",
    "    download_images(image_urls, IMAGES_DIR)\n",
    "    print(\"Image download process complete.\")\n",
    "\n",
    "    # 2. Define the feature extractor model\n",
    "    print(\"Building image feature extractor model...\")\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False, weights='imagenet', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "    image_input = tf.keras.layers.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    x = base_model(image_input, training=False)\n",
    "    pooled_output = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    feature_extractor = tf.keras.Model(image_input, pooled_output, name=\"image_feature_extractor\")\n",
    "    print(\"Model built.\")\n",
    "\n",
    "    # --- FIX: Verify which images actually exist before processing ---\n",
    "    print(\"Verifying downloaded images...\")\n",
    "    all_paths = df['image_path'].tolist()\n",
    "    existing_paths = [p for p in all_paths if os.path.exists(p)]\n",
    "    print(f\"Found {len(existing_paths)} existing images out of {len(all_paths)} total.\")\n",
    "\n",
    "    if len(existing_paths) > 0:\n",
    "        # 3. Create a tf.data pipeline ONLY for existing images\n",
    "        def load_image_for_prediction(path):\n",
    "            try:\n",
    "                image = tf.io.read_file(path)\n",
    "                image = tf.io.decode_jpeg(image, channels=3)\n",
    "                image = tf.image.resize(image, IMAGE_SIZE)\n",
    "                image = tf.keras.applications.resnet50.preprocess_input(image)\n",
    "                return image\n",
    "            except:\n",
    "                return tf.zeros((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "        print(\"Creating a tf.data.Dataset for existing image paths...\")\n",
    "        image_path_ds = tf.data.Dataset.from_tensor_slices(existing_paths)\n",
    "        image_ds = image_path_ds.map(load_image_for_prediction, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        image_ds = image_ds.batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        # 4. Generate embeddings for existing images\n",
    "        print(\"Generating image embeddings for existing items...\")\n",
    "        partial_image_embeddings = feature_extractor.predict(image_ds, verbose=1)\n",
    "\n",
    "        # 5. Map embeddings back to the original dataframe order\n",
    "        print(\"Mapping generated embeddings back to the original 75,000-item order...\")\n",
    "        embedding_dim = partial_image_embeddings.shape[1]\n",
    "        embedding_map = dict(zip(existing_paths, partial_image_embeddings))\n",
    "\n",
    "        final_image_embeddings = np.zeros((len(df), embedding_dim))\n",
    "        for i, path in enumerate(df['image_path']):\n",
    "            if path in embedding_map:\n",
    "                final_image_embeddings[i] = embedding_map[path]\n",
    "\n",
    "        print(f\"Final image embeddings generated with shape: {final_image_embeddings.shape}\")\n",
    "        np.save(IMAGE_EMBEDDINGS_FILE, final_image_embeddings)\n",
    "        print(f\"Image embeddings saved to {IMAGE_EMBEDDINGS_FILE}\")\n",
    "\n",
    "        # Assign to the main variable for the next cell\n",
    "        image_embeddings = final_image_embeddings\n",
    "    else:\n",
    "        print(\"Error: No images were found. Cannot generate embeddings.\")\n",
    "        # Create a dummy zero array if no images are found\n",
    "        image_embeddings = np.zeros((len(df), 2048)) # 2048 is ResNet50's output dim\n",
    "\n",
    "else:\n",
    "    print(f\"Loading pre-computed image embeddings from {IMAGE_EMBEDDINGS_FILE}...\")\n",
    "    image_embeddings = np.load(IMAGE_EMBEDDINGS_FILE)\n",
    "    print(f\"Image embeddings loaded with shape: {image_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding shape: (75000, 384)\n",
      "Image embedding shape: (75000, 2048)\n",
      "IPQ features shape: (75000, 1)\n",
      "Final feature matrix X shape: (75000, 2433)\n",
      "Target vector y shape: (75000,)\n",
      "\n",
      "Starting 5-fold cross-validation with LightGBM...\n",
      "--- Fold 1/5 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.559961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 620242\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2433\n",
      "[LightGBM] [Info] Start training from score 2.740904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/holy_SHIT/ML_AWS_CHALLENGE/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 RMSE: 0.7443431458757918\n",
      "--- Fold 2/5 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.977747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 620247\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2433\n",
      "[LightGBM] [Info] Start training from score 2.738173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/holy_SHIT/ML_AWS_CHALLENGE/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 RMSE: 0.7227761915172778\n",
      "--- Fold 3/5 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.662333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 620245\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2433\n",
      "[LightGBM] [Info] Start training from score 2.741725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/holy_SHIT/ML_AWS_CHALLENGE/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 RMSE: 0.7224129549366619\n",
      "--- Fold 4/5 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.201671 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 620244\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2433\n",
      "[LightGBM] [Info] Start training from score 2.737836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/holy_SHIT/ML_AWS_CHALLENGE/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 RMSE: 0.7145844044843473\n",
      "--- Fold 5/5 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.924996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 620241\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2433\n",
      "[LightGBM] [Info] Start training from score 2.737449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/holy_SHIT/ML_AWS_CHALLENGE/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 RMSE: 0.7291847929954292\n",
      "\n",
      "Average RMSE across all folds: 0.7266602979619016\n",
      "Saving the 5 trained models...\n",
      "Models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Combine Features and Train LightGBM Model ---\n",
    "\n",
    "# Reshape ipq to be a 2D array for concatenation\n",
    "ipq_features = df['ipq'].values.reshape(-1, 1)\n",
    "\n",
    "print(f\"Text embedding shape: {text_embeddings.shape}\")\n",
    "print(f\"Image embedding shape: {image_embeddings.shape}\")\n",
    "print(f\"IPQ features shape: {ipq_features.shape}\")\n",
    "\n",
    "# Combine all features into a single matrix X\n",
    "X = np.concatenate([text_embeddings, image_embeddings, ipq_features], axis=1)\n",
    "y = df['log_price'].values\n",
    "\n",
    "print(f\"Final feature matrix X shape: {X.shape}\")\n",
    "print(f\"Target vector y shape: {y.shape}\")\n",
    "\n",
    "# --- Cross-Validation Training ---\n",
    "N_SPLITS = 5\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_predictions = np.zeros(X.shape[0])\n",
    "oof_models = []\n",
    "oof_scores = []\n",
    "\n",
    "print(f\"\\nStarting {N_SPLITS}-fold cross-validation with LightGBM...\")\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Define the LightGBM model\n",
    "    lgbm = lgb.LGBMRegressor(\n",
    "        random_state=42,\n",
    "        n_estimators=1000, # High number of estimators, will be stopped by early stopping\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    lgbm.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)] # Stop if validation score doesn't improve for 100 rounds\n",
    "    )\n",
    "\n",
    "    # Predict on the validation set\n",
    "    val_preds = lgbm.predict(X_val)\n",
    "    oof_predictions[val_index] = val_preds\n",
    "\n",
    "    # Evaluate the model\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    oof_scores.append(rmse)\n",
    "    oof_models.append(lgbm)\n",
    "    print(f\"Fold {fold+1} RMSE: {rmse}\")\n",
    "\n",
    "# --- Final Performance ---\n",
    "mean_oof_rmse = np.mean(oof_scores)\n",
    "print(f\"\\nAverage RMSE across all folds: {mean_oof_rmse}\")\n",
    "\n",
    "# --- Save the trained models ---\n",
    "print(\"Saving the 5 trained models...\")\n",
    "for i, model in enumerate(oof_models):\n",
    "    joblib.dump(model, f'artifacts/lgbm_model_fold_{i+1}.pkl')\n",
    "print(\"Models saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
